import os
import logging
import requests
import time
import yaml
import json
from openai import OpenAI
from assistant_robot.common.gpt_client.base_client import BaseLLMClient
# from base_client import BaseLLMClient

class QwenClient(BaseLLMClient):
    """
    Qwen (é˜¿é‡Œäº‘ç™¾ç‚¼ / DashScope) æ¨¡å‹å®¢æˆ·ç«¯
    ç»§æ‰¿ BaseLLMClientï¼Œç‹¬ç«‹å®ç° Qwen API è°ƒç”¨é€»è¾‘
    """

    def __init__(self, config: dict):
        # ğŸš« å¼ºåˆ¶å…³é—­æ‰€æœ‰ä»£ç†
        for key in ["HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "http_proxy", "https_proxy", "all_proxy", "FTP_PROXY", "ftp_proxy"]:
            os.environ.pop(key, None)
        os.environ["NO_PROXY"] = "*"

        # ğŸ§© ç¦ç”¨ requests å¯¹ç³»ç»Ÿä»£ç†çš„ä¿¡ä»»
        session = requests.Session()
        session.trust_env = False
        """
        åˆå§‹åŒ– Qwen å®¢æˆ·ç«¯
        :param config: å« "api"ã€"templates" ç­‰å­—æ®µçš„é…ç½®
        """
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(config)
        self.logger = logging.getLogger(self.__class__.__name__)

        provider_cfg = config.get("api", {})
        self.cfg = provider_cfg
        self.templates_path = config.get("templates_path", "")
        self.template_map = config.get("templates", {})

        self.messages_cache =[]
        self.sys_tmp = False

        # è¯»å– API key (ä¼˜å…ˆçº§: key > key_path > key_env_var)
        if "key" in provider_cfg:
            self.api_key = provider_cfg["key"]
        elif "key_path" in provider_cfg:
            with open(provider_cfg["key_path"], "r") as f:
                self.api_key = f.read().strip()
        elif "key_env_var" in provider_cfg:
            self.api_key = os.getenv(provider_cfg["key_env_var"])
        else:
            raise ValueError("No Qwen API key provided via key, key_path, or key_env_var")

        # ---- åŸºç¡€é…ç½® ----
        # åŒ—äº¬åœ°åŸŸï¼š https://dashscope.aliyuncs.com/compatible-mode/v1
        # æ–°åŠ å¡åœ°åŸŸï¼š https://dashscope-intl.aliyuncs.com/compatible-mode/v1
        self.base_url = provider_cfg.get(
            "base_url",
            "https://dashscope.aliyuncs.com/compatible-mode/v1"
        )

        models = provider_cfg.get("models", [])
        self.default_model = provider_cfg.get("default_model") or (models[0]["name"] if models else "qwen-plus")
        self.default_temperature = provider_cfg.get("default_temperature", 0.7)
        self.timeout = provider_cfg.get("timeout", 60)

        # âœ… ä½¿ç”¨ OpenAI SDK åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=self.timeout
        )

        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.info(f"Initialized QwenClient with model: {self.default_model}")

    # ---------------------------------------------------------
    #  æ„å»º payload
    # ---------------------------------------------------------
    def _build_payload(self, messages, **kwargs) -> dict:
        """
        æ„å»º Qwen ChatCompletion è¯·æ±‚ä½“
        """
        model = kwargs.get("model", self.default_model)
        temperature = kwargs.get("temperature", self.default_temperature)
        
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
        }

        # print("*****************kwargsï¼š",kwargs)
        # é€šç”¨å‚æ•°
        for key in ["max_tokens", "top_p", "presence_penalty", "stop"]:
            if key in kwargs:
                payload[key] = kwargs[key]
        if "extra_body" in kwargs:
            payload["extra_body"] = kwargs["extra_body"]
            # extra_body={"enable_search": True, "enable_thinking": True}

        if "stream" in kwargs:
            payload["stream"] = kwargs["stream"]
        
        if "response_format" in kwargs:
            payload["response_format"] = kwargs["response_format"]

        if "tools" in kwargs:
            payload["tools"] = kwargs["tools"]

        if "parallel_tool_calls" in kwargs:
            payload["parallel_tool_calls"] = kwargs["parallel_tool_calls"]

        return payload
    
    # ---------------------------------------------------------
    #  æ‰§è¡Œè¯·æ±‚
    # ---------------------------------------------------------
    # def _raw_request(self, payload: dict) -> dict:
    #     """
    #     å‘é€ HTTP è¯·æ±‚åˆ° Qwen DashScope å…¼å®¹æ¥å£
    #     """
        
    #     url = f"{self.base_url.rstrip('/')}/chat/completions"
    #     headers = {
    #         "Authorization": f"Bearer {self.api_key}",
    #         "Content-Type": "application/json",
    #     }

    #     self.logger.debug(f"Qwen Request URL: {url}")
    #     self.logger.debug(f"Payload: {payload}")

    #     resp = requests.post(url, json=payload, headers=headers, timeout=self.timeout)
    #     try:
    #         resp.raise_for_status()
    #     except requests.HTTPError as e:
    #         self.logger.error(f"Qwen API error: {e}, response: {resp.text}")
    #         raise

    #     return resp.json()
    
    def _raw_request(self, payload: dict) -> dict:
        """
        è°ƒç”¨ OpenAI SDK (Qwen å…¼å®¹æ¨¡å¼)
        """
        start = time.time()
        try:
            completion = self.client.chat.completions.create(**payload)
            resp = completion.model_dump()  # è½¬æˆæ™®é€š dict
        except Exception as e:
            self.logger.error(f"Qwen SDK request error: {e}")
            raise
        elapsed = time.time() - start
        self.logger.info(f"Qwen SDK request took {elapsed:.2f}s")
        return resp
    
    def _parse_response(self, resp: dict) -> str:
        """
        æå–å†…å®¹
        """
        try:
            return resp["choices"][0]["message"]["content"]
        except Exception as e:
            self.logger.error(f"Failed to parse Qwen response: {e}")
            raise
    
    def chat(self, template_key: str,user_instruction:str, **kwargs) -> str:
        """
        é€šç”¨æ¥å£ï¼šæ ¹æ®æ¨¡æ¿ key + å‚æ•°æ¸²æŸ“ promptï¼Œ
        æ„å»º payloadï¼Œå‘é€è¯·æ±‚å¹¶è§£æè¿”å›å†…å®¹ã€‚
        """
        messages = []
        prompt_yaml = self.prompt_mgr.render_ymal(template_key, **kwargs)

        system_prompt = prompt_yaml["messages"][0]["content"]
        messages.append({"role": "system", "content":system_prompt})
       
        # 2. æ„å»ºè¯·æ±‚ä½“
        if "history" in kwargs:
            # self.messages_cache.append({"role": "assistant", "content": kwargs["history"]})
            messages.append({"role": "assistant", "content": kwargs["history"]})

        # self.messages_cache.append({"role": "user", "content": user_instruction})
        messages.append({"role": "user", "content": user_instruction})

        payload = self._build_payload(messages,**kwargs)
        # 3. å‘é€è¯·æ±‚
        start = time.time()
        resp = self._request_with_retry(payload)
        elapsed = time.time() - start
        self.logger.info(f"[{self.__class__.__name__}] {template_key} took {elapsed:.2f}s")
        # 4. è§£æå“åº”
        return self._parse_response(resp)


if __name__ == "__main__":
    # # ğŸš« å¼ºåˆ¶å…³é—­æ‰€æœ‰ä»£ç†
    # for key in ["HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "http_proxy", "https_proxy", "all_proxy", "FTP_PROXY", "ftp_proxy"]:
    #     os.environ.pop(key, None)
    # os.environ["NO_PROXY"] = "*"

    # # ğŸ§© ç¦ç”¨ requests å¯¹ç³»ç»Ÿä»£ç†çš„ä¿¡ä»»
    # session = requests.Session()
    # session.trust_env = False

    # # ğŸ§ª æœ¬åœ°æµ‹è¯•
    # cfg = {
    #     "api": {
    #         "key_path": "/home/maintenance/Code/instruction/assistant_ws_v2/src/assistant_robot/config/keys/qwen_key.txt",
    #         "default_model": "qwen-flash",
    #         "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
    #         "timeout": 30,
    #     }
    # }

    # qwen = QwenClient(cfg)

    # prompt = "è¯·æŠ½å–ç”¨æˆ·çš„å§“åã€å¹´é¾„ã€é‚®ç®±ã€çˆ±å¥½ä¿¡æ¯ï¼Œä»¥JSONæ ¼å¼è¿”å›ã€‚\næˆ‘å«å¼ ä¸‰ï¼Œä»Šå¹´34å²ï¼Œé‚®ç®±æ˜¯zhangsan@example.comï¼Œå¹³æ—¶å–œæ¬¢æ‰“ç¯®çƒå’Œæ—…æ¸¸ã€‚"

    # payload = qwen._build_payload(prompt)
    # result = qwen._raw_request(payload)

    # print(result["choices"][0]["message"]["content"])



    import sys
    from factory import ProviderFactory
    sys.path.append("..") 
    from utils import load_yaml_test

    # gpt_cfg = load_yaml_test("/home/maintenance/Code/instruction/assistant_ws_v2/src/assistant_robot/config/gpt_config.yaml")
    gpt_cfg = load_yaml_test("gpt_config.yaml")
    # print(gpt_cfg)
    qwen_client = ProviderFactory.create(gpt_cfg)

    result = qwen_client.chat(
        template_key="llm_qa",  # æˆ–ä»»æ„æ¨¡æ¿  llm_planner  llm_intent   llm_clarifier  llm_qa
        # user_instruction="å°†ç»¿è‰²æ¡Œå­ä¸Šçš„ä¸¤ä¸ªç‹®å­æ”¾åˆ°é»‘è‰²æ¡Œå­ä¸Šï¼Œç„¶åå°†åœ†æ¡Œä¸Šçš„ä¸‰ä¸ªæé¾™æ”¾åˆ°ç»¿è‰²æ¡Œå­ä¸Š",
        # user_instruction="å…ˆåˆ°ç™½è‰²æ¡Œå­ï¼Œç„¶ååˆ°é»‘è‰²æ¡Œå­ï¼Œç„¶ååˆ°ç”µè§†æŸœï¼Œæœ€åå›åˆ°æ²™å‘",
        # user_instruction="å»ç™½è‰²æ¡Œå­ï¼Œç„¶åå°†æ¡Œå­ä¸Šçš„ç‹®å­æ”¾åˆ°æ²™å‘ï¼Œç„¶ååˆ°å›åˆ°ç™½è‰²æ¡Œå­",

        # user_instruction="è®²ä¸ªç¬‘è¯å§",
        user_instruction="ä»Šå¤©æ˜¯ä»€ä¹ˆæ—¥å­ï¼Ÿç°åœ¨å‡ ç‚¹äº†ï¼ŸåŒ—äº¬ç°åœ¨çš„æ°”æ¸©æ€ä¹ˆæ ·ï¼Ÿ",
        # user_instruction="å°†æ¡Œå­ä¸Šçš„ç©å¶æ”¾åˆ°ç”µè§†æŸœä¸Š", 

        # user_instruction="å°†è¿™ä¸ªç©å¶æ”¾åˆ°é‚£é‡Œ",
        # user_instruction="å°†æ²™å‘ä¸Šçš„ç‹®å­æ‹¿ç»™æˆ‘",
        # user_instruction="æˆ‘æƒ³è¦é‚£ä¸ªé»„è‰²çš„ç©å¶",
        # user_instruction="å°†ç™½æ¡Œå­ä¸Šçš„å‡ ä¸ªé¸­å­éƒ½æ”¾åˆ°æ²™å‘ä¸Š",

        # user_instruction="æˆ‘è¦æ²™å‘ä¸Šçš„é¸­å­",
        # user_instruction="å°†æ²™å‘ä¸Šçš„æé¾™æ”¾åˆ°ç™½è‰²æ¡Œå­",
        # user_instruction="å¯¼èˆªåˆ°ç™½è‰²æ¡Œå­",
        extra_body={
            # "enable_reasoning": True,   # å¼€å¯â€œæ€è€ƒæ¨¡å¼â€
            "enable_search": True,
                # "search_options": {
                #     "forced_search": True  # å¼ºåˆ¶è”ç½‘æœç´¢
                # }
        },
                
        # response_format={"type": "json_object"},
        # stream=True
    )

    print("âœ… Qwen è¿”å›ï¼š")
    # print("âœ… Qwen Plus è¿”å›ï¼š")
    # print("âœ… Qwen3 Max è¿”å›ï¼š")
    # print("âœ… Deepseek-R1 è¿”å›ï¼š")
    # print("âœ… Kimi è¿”å›ï¼š")
    print(result)
